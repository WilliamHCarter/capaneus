{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import torch\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11}\n",
    "ydim = [i for i in range(19, 0, -1)]\n",
    "# print(xdim)\n",
    "# print(ydim)\n",
    "def get_hold_coords(hold, xdim = xdim, ydim = ydim):\n",
    "    col = xdim[hold[0]] -1\n",
    "    row = ydim[int(hold[1:])] - 1\n",
    "    return (row, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542\n",
      "Method                                                      Feet follow hands\n",
      "Name                                                         ONE ARMED BANDIT\n",
      "Grade                                                                      8A\n",
      "UserGrade                                                                  8A\n",
      "MoonBoardConfiguration                                                    NaN\n",
      "MoonBoardConfigurationId                                                    0\n",
      "Setter                      {'Id': '89b67a5a-62d2-46df-95d6-4a5e25b0c361',...\n",
      "FirstAscender                                                           False\n",
      "Rating                                                                      0\n",
      "UserRating                                                                  5\n",
      "Repeats                                                                     0\n",
      "Attempts                                                                    0\n",
      "Holdsetup                   {'Id': 1, 'Description': 'MoonBoard 2016', 'Se...\n",
      "IsBenchmark                                                              True\n",
      "IsMaster                                                                False\n",
      "IsAssessmentProblem                                                     False\n",
      "ProblemType                                                               NaN\n",
      "Moves                       [{'Id': 2018503, 'Description': 'E13', 'IsStar...\n",
      "Holdsets                    [{'Id': 0, 'Description': 'Original School Hol...\n",
      "Locations                   [{'Id': 0, 'Holdset': None, 'Description': 'A1...\n",
      "RepeatText                                                                NaN\n",
      "NumberOfTries                                                             NaN\n",
      "NameForUrl                                                   one-armed-bandit\n",
      "Upgraded                                                                False\n",
      "Downgraded                                                              False\n",
      "Id                                                                     354762\n",
      "ApiId                                                                       0\n",
      "DateInserted                                            /Date(1572472915233)/\n",
      "DateUpdated                                                               NaN\n",
      "DateDeleted                                                               NaN\n",
      "DateTimeString                                              30 Oct 2019 22:01\n",
      "Name: 10, dtype: object\n",
      "[{'Id': 2018503, 'Description': 'E13', 'IsStart': False, 'IsEnd': False}, {'Id': 2018504, 'Description': 'A16', 'IsStart': False, 'IsEnd': False}, {'Id': 2018505, 'Description': 'C18', 'IsStart': False, 'IsEnd': True}, {'Id': 2018506, 'Description': 'H12', 'IsStart': False, 'IsEnd': False}, {'Id': 2018507, 'Description': 'J2', 'IsStart': True, 'IsEnd': False}, {'Id': 2018508, 'Description': 'K6', 'IsStart': True, 'IsEnd': False}, {'Id': 2018509, 'Description': 'K9', 'IsStart': False, 'IsEnd': False}]\n"
     ]
    }
   ],
   "source": [
    "moonboard_2016_filename = \"../data/mb_problem_holds.csv\"\n",
    "moonboard_2016_size = (18,11)\n",
    "df = pd.read_csv(moonboard_2016_filename)\n",
    "print(len(df))\n",
    "climb = df.iloc[10]\n",
    "print(climb)\n",
    "print(climb['Moves'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split Data\n",
    "# def split_csv_and_save(input_file, output_dir, train_size=0.8, random_state=None):\n",
    "#     # Create the output directory if it doesn't exist\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     # Read the CSV file\n",
    "#     data = pd.read_csv(input_file)\n",
    "    \n",
    "#     # Split the data into train and test sets\n",
    "#     train_data, test_data = train_test_split(data, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "#     # Define the paths for train and test files\n",
    "#     train_file = os.path.join(output_dir, \"train.csv\")\n",
    "#     test_file = os.path.join(output_dir, \"test.csv\")\n",
    "    \n",
    "#     # Save train and test data to CSV files\n",
    "#     train_data.to_csv(train_file, index=False)\n",
    "#     test_data.to_csv(test_file, index=False)\n",
    "\n",
    "# # Example usage\n",
    "# moonboard_2016_filename = \"../data/mb_problem_holds.csv\"\n",
    "# output_directory = \"../modeldata/splits\"\n",
    "# split_csv_and_save(moonboard_2016_filename, output_directory, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " 3.0)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_grade_order = ['4', '5', '5+', '6A', '6A+', '6B', '6B+', '6C', '6C+', '7A', '7A+',\n",
    "                       '7B', '7B+', '7C', '7C+', '8A', '8A+', '8B', '8B+']\n",
    "grade_bounds = ('6B+', '8A')\n",
    "scale_min = french_grade_order.index(grade_bounds[0])\n",
    "scale_max = french_grade_order.index(grade_bounds[1])\n",
    "scale_bounds = (scale_min, scale_max)\n",
    "scale_size = scale_max - scale_min\n",
    "\n",
    "class ClimbDataset(Dataset):\n",
    "    def __init__(self, climbs_filename, board_size, scale_bounds):\n",
    "        self.climb_df = pd.read_csv(climbs_filename)\n",
    "        self.board_size = board_size\n",
    "        self.scale_min = scale_bounds[0]\n",
    "        self.scale_max = scale_bounds[1]\n",
    "        self.scale_size = self.scale_max - self.scale_min + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.climb_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        climb = self.climb_df.iloc[idx]\n",
    "        grade = climb['Grade']\n",
    "        moves = climb['Moves']\n",
    "        moves = moves.replace(\"'\", '\"')\n",
    "        moves = moves.replace(\"True\", 'true')\n",
    "        moves = moves.replace(\"False\", 'false')\n",
    "        moves = json.loads(moves)\n",
    "        holds = np.zeros(self.board_size)\n",
    "        for hold in moves:\n",
    "            x, y = get_hold_coords(hold['Description'].upper())\n",
    "            holds[x, y] = 1\n",
    "        curr_grade_ind = french_grade_order.index(grade)\n",
    "        if self.scale_min > curr_grade_ind :\n",
    "            grade_num = 0\n",
    "        elif self.scale_max < curr_grade_ind :\n",
    "            grade_num = self.scale_max - self.scale_min\n",
    "        else:\n",
    "            grade_num = curr_grade_ind - self.scale_min\n",
    "\n",
    "        # one_hot = np.zeros(self.scale_size)\n",
    "        # one_hot[grade_num] = 1\n",
    "        return holds.astype(np.float32), float(grade_num)\n",
    "train_dataset = ClimbDataset(\"../modeldata/splits/train.csv\", moonboard_2016_size, scale_bounds)\n",
    "test_dataset = ClimbDataset(\"../modeldata/splits/test.csv\", moonboard_2016_size, scale_bounds)\n",
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "input_size = moonboard_2016_size[0] * moonboard_2016_size[1]\n",
    "output_size = scale_size\n",
    "class NeuralNetwork_Orig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_Orig, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork_Orig().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        y = y.float()\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        # print(y)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(loss.dtype)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, error = 0, 0\n",
    "    with torch.no_grad():  # Why do we do this?\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y)\n",
    "            error += ((pred.squeeze() - y).abs()).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    error /= size\n",
    "    return (error, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avery/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/avery/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/avery/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.448611746208772\n",
      "3.448611746208772\n",
      "At Epoch 0 model has Accuracy: 390.5%, Avg loss: 24.316093 \n",
      "\n",
      "At Epoch 0 new best model found with Accuracy: 390.5%, Avg loss: 24.316093 \n",
      "\n",
      "3.438440459429805\n",
      "3.428306456250902\n",
      "3.4182232334762466\n",
      "3.408185152754222\n",
      "3.3982020283406107\n",
      "3.38827683798986\n",
      "3.378410127917283\n",
      "3.3685978279378066\n",
      "3.358821789607165\n",
      "3.349076709350868\n",
      "3.3393669745112677\n",
      "3.3297056766360242\n",
      "3.320093291461055\n",
      "3.3105093623419006\n",
      "3.3009480394887594\n",
      "3.29140948148027\n",
      "3.2819052997967937\n",
      "3.272431565212047\n",
      "3.2629835732264003\n",
      "3.2535612533604574\n",
      "3.244166931434261\n",
      "3.234798264007921\n",
      "3.2254542115103564\n",
      "3.2161279902997797\n",
      "3.206815036835351\n",
      "3.1975142410665955\n",
      "3.188230483691632\n",
      "3.17896008216206\n",
      "3.169697503845356\n",
      "3.1604422377659045\n",
      "3.151193200303005\n",
      "3.1419546906997646\n",
      "3.132725184839262\n",
      "3.1235119244793546\n",
      "3.114315138678077\n",
      "3.105136818478344\n",
      "3.095960295503189\n",
      "3.0867876400848457\n",
      "3.0776191341408956\n",
      "3.0684546719522454\n",
      "3.059295654296875\n",
      "3.050150107015914\n",
      "3.041002859419543\n",
      "3.0318559025506775\n",
      "3.0227079854000523\n",
      "3.013559407505097\n",
      "3.004404926960793\n",
      "2.9952531510632805\n",
      "2.9860997277099046\n",
      "2.976966153116204\n",
      "2.976966153116204\n",
      "2.967831397992634\n",
      "2.9586909868975986\n",
      "2.9495434309538737\n",
      "2.9403853823901875\n",
      "2.9312136784436795\n",
      "2.922040688248209\n",
      "2.9128600422284343\n",
      "2.903673608088328\n",
      "2.894474584566253\n",
      "2.8852675352030484\n",
      "2.8760583714579875\n",
      "2.866847216670012\n",
      "2.8576216664655654\n",
      "2.848381861803438\n",
      "2.83913412820935\n",
      "2.8298710124872573\n",
      "2.820591686504267\n",
      "2.811298458460572\n",
      "2.80199048260343\n",
      "2.792665397873101\n",
      "2.783323442137544\n",
      "2.773966500720581\n",
      "2.764591516578445\n",
      "2.755197652768318\n",
      "2.7457850061993674\n",
      "2.736353726640309\n",
      "2.726903796471294\n",
      "2.717435021873985\n",
      "2.709147288122023\n",
      "2.7030189197002876\n",
      "2.6968772042430853\n",
      "2.690721886262607\n",
      "2.684552798370291\n",
      "2.6783686014576267\n",
      "2.6721691545658244\n",
      "2.665955514885812\n",
      "2.6597272947809163\n",
      "2.653484097804539\n",
      "2.647227641891938\n",
      "2.6409576098858345\n",
      "2.6346658878458564\n",
      "2.6283562904692688\n",
      "2.622032614833488\n",
      "2.6156985963464607\n",
      "2.6093515920308668\n",
      "2.6029911878202583\n",
      "2.5966171370367532\n",
      "2.590230725929313\n",
      "2.5838308532573886\n",
      "2.5774177833187113\n",
      "2.5774177833187113\n",
      "At Epoch 100 model has Accuracy: 313.9%, Avg loss: 15.694824 \n",
      "\n",
      "2.57099021224447\n",
      "2.5645479814560255\n",
      "2.5580912054823948\n",
      "2.551623654971222\n",
      "2.5451416385641825\n",
      "2.5386451738811253\n",
      "2.532136956758918\n",
      "2.525615559989806\n",
      "2.5190799087630134\n",
      "2.512530478814458\n",
      "2.5059672349044413\n",
      "2.499391181364346\n",
      "2.4928042475676153\n",
      "2.486205808009617\n",
      "2.4795942504741855\n",
      "2.472969909738448\n",
      "2.46633293557112\n",
      "2.45968370679895\n",
      "2.4530222762814846\n",
      "2.44634913737449\n",
      "2.4396651710704074\n",
      "2.4329703421295377\n",
      "2.4262651174778753\n",
      "2.4195496733139072\n",
      "2.412824732051435\n",
      "2.406089853194239\n",
      "2.3993455741477066\n",
      "2.3925877190076728\n",
      "2.385820622256942\n",
      "2.3790451120284084\n",
      "2.37226238647179\n",
      "2.3654724191573147\n",
      "2.358675465572789\n",
      "2.3518727679175537\n",
      "2.345063533298413\n",
      "2.3382488805757657\n",
      "2.3314305100650214\n",
      "2.3246089415517193\n",
      "2.3177850207886026\n",
      "2.310958976833705\n",
      "2.3041303868106553\n",
      "2.2972995326370347\n",
      "2.290471858846123\n",
      "2.2836447665102213\n",
      "2.2768170486696873\n",
      "2.2699908020865283\n",
      "2.263165832942408\n",
      "2.2563429517503697\n",
      "2.249523832396051\n",
      "2.2427084396397543\n",
      "2.2427084396397543\n",
      "2.235897011349438\n",
      "2.2310639925421523\n",
      "2.227482661363985\n",
      "2.2239043872295845\n",
      "2.2203300070817695\n",
      "2.2167595825900106\n",
      "2.213193474961208\n",
      "2.209631904443472\n",
      "2.2060754965414353\n",
      "2.2025246829413927\n",
      "2.1989798072303968\n",
      "2.195441424433684\n",
      "2.191909992667324\n",
      "2.1883857938488966\n",
      "2.184869136325757\n",
      "2.18136141206596\n",
      "2.1778628765573127\n",
      "2.17437383814717\n",
      "2.1708948254309957\n",
      "2.1674253098133254\n",
      "2.1639655996415135\n",
      "2.160516813775959\n",
      "2.15707926937394\n",
      "2.1536539707668383\n",
      "2.150240917954654\n",
      "2.146840454524439\n",
      "2.143452959302942\n",
      "2.1400788111169122\n",
      "2.136718556181663\n",
      "2.1333728728613734\n",
      "2.13004209593317\n",
      "2.126726648273424\n",
      "2.123426732510397\n",
      "2.120142912479251\n",
      "2.116875426047944\n",
      "2.11362488110126\n",
      "2.1103916476560243\n",
      "2.107176078109213\n",
      "2.10397772315468\n",
      "2.1007978602314656\n",
      "2.0976368857861667\n",
      "2.094495997968502\n",
      "2.0913748003318733\n",
      "2.0882737157526523\n",
      "2.0851931054377393\n",
      "2.082133172015395\n",
      "2.0790944969406304\n",
      "2.0760776528585314\n",
      "2.0730824459507615\n",
      "2.0701084885806464\n",
      "2.0701084885806464\n",
      "At Epoch 200 model has Accuracy: 259.3%, Avg loss: 10.057080 \n",
      "\n",
      "2.0671568643688882\n",
      "2.0642278904727647\n",
      "2.061321857619781\n",
      "2.0584391270168383\n",
      "2.0555798572425754\n",
      "2.052744356644346\n",
      "2.0499327661809414\n",
      "2.047145438249337\n",
      "2.0443826811968866\n",
      "2.041644609552608\n",
      "2.0389312585561985\n",
      "2.036243086323727\n",
      "2.0335802426239082\n",
      "2.0309427626964403\n",
      "2.028330875599357\n",
      "2.0257449161097854\n",
      "2.023184681599465\n",
      "2.0206504011264297\n",
      "2.018142365418185\n",
      "2.0156606361442018\n",
      "2.0132053983128926\n",
      "2.010776528585315\n",
      "2.0083743705485215\n",
      "2.005998739194099\n",
      "2.003649960489251\n",
      "2.0013278670454135\n",
      "1.999032643870999\n",
      "1.9967642821560831\n",
      "1.994522746660968\n",
      "1.9923080373856543\n",
      "1.9901203305286292\n",
      "1.987959220833371\n",
      "1.985824708299879\n",
      "1.9837168281678514\n",
      "1.9816352985197072\n",
      "1.982048149196986\n",
      "1.9825877394466973\n",
      "1.9831202905668122\n",
      "1.9836457585077087\n",
      "1.984164213748782\n",
      "1.9846756739098812\n",
      "1.9851801125612325\n",
      "1.9856777147112488\n",
      "1.9861683834507613\n",
      "1.9866520923499973\n",
      "1.9871290176074445\n",
      "1.9875991327933296\n",
      "1.9880624819572748\n",
      "1.9885190650992801\n",
      "1.9889689086491187\n",
      "1.9889689086491187\n",
      "1.9894121976152028\n",
      "1.9898487910387423\n",
      "1.990278865118225\n",
      "1.9907023934238777\n",
      "1.9911194023854737\n",
      "1.991530129870972\n",
      "1.9919344437315063\n",
      "1.9923323792067742\n",
      "1.9927241301151126\n",
      "1.9931096259771264\n",
      "1.9934890429913035\n",
      "1.9938623018683241\n",
      "1.9942296404761475\n",
      "1.9945909971453026\n",
      "1.9949464247353357\n",
      "1.9952960377752644\n",
      "1.9956398891246345\n",
      "1.9959779347338247\n",
      "1.9963103772310957\n",
      "1.9966372606660698\n",
      "1.9969586026585955\n",
      "1.9972744736880683\n",
      "1.9975849618537322\n",
      "1.9978902081143775\n",
      "1.998190036271516\n",
      "1.9984847458625776\n",
      "1.9987742840280158\n",
      "1.9990588269663188\n",
      "1.99933832181794\n",
      "1.9996128566821232\n",
      "1.9998825901375081\n",
      "2.0001474164650017\n",
      "2.000407573532563\n",
      "2.000662990860796\n",
      "2.000913809408492\n",
      "2.0011600996550456\n",
      "2.0014018880302307\n",
      "2.001639245013442\n",
      "2.00187231156347\n",
      "2.0021009114818265\n",
      "2.0023253707357145\n",
      "2.00254568051521\n",
      "2.0027618672500855\n",
      "2.0029740542792833\n",
      "2.003182179933332\n",
      "2.0033864820801908\n",
      "2.003586793331296\n",
      "2.0037834396538505\n",
      "2.003976315328761\n",
      "2.0041654908354234\n",
      "2.0041654908354234\n",
      "At Epoch 300 model has Accuracy: 244.6%, Avg loss: 8.633797 \n",
      "\n",
      "2.0043511247524766\n",
      "2.0045331289806763\n",
      "2.0047117149582085\n",
      "2.0048867505362074\n",
      "2.0050584735826313\n",
      "2.005226919337178\n",
      "2.0053920701799988\n",
      "2.0055540318301865\n",
      "2.005712848337363\n",
      "2.005868528511453\n",
      "2.006021142831851\n",
      "2.006170805827575\n",
      "2.0063175086886997\n",
      "2.0064612337953776\n",
      "2.00660216615602\n",
      "2.0067403057706272\n",
      "2.0068756878788974\n",
      "2.0070083389106026\n",
      "2.0071383557749125\n",
      "2.0072657913313727\n",
      "2.0073906631998324\n",
      "2.007513033049762\n",
      "2.0076329889804057\n",
      "2.007750451702444\n",
      "2.007865500505196\n",
      "2.0079783468268486\n",
      "2.008088823278837\n",
      "2.008197123679498\n",
      "2.0083031687395128\n",
      "2.0084069936985784\n",
      "2.008508757135334\n",
      "2.008608503099402\n",
      "2.008706125871689\n",
      "2.008801757601062\n",
      "2.0088954511470662\n",
      "2.0089872065097025\n",
      "2.0090770941683656\n",
      "2.0091651141230558\n",
      "2.009251310423395\n",
      "2.009335727119005\n",
      "2.0094184082595086\n",
      "2.0094993450349805\n",
      "2.0095785814950426\n",
      "2.0096561793091667\n",
      "2.0097323058659153\n",
      "2.009806696867557\n",
      "2.0098795549423536\n",
      "2.009950950569699\n",
      "2.0100208485098965\n",
      "2.01008925757287\n",
      "2.01008925757287\n",
      "2.0101562129983175\n",
      "2.0102217940755582\n",
      "2.0102860008045926\n",
      "2.0103488596151937\n",
      "2.010410405747059\n",
      "2.010470586340642\n",
      "2.0105296304539775\n",
      "2.010587335458804\n",
      "2.0106438775536093\n",
      "2.0106991510193013\n",
      "2.010753323244443\n",
      "2.0108064030389587\n",
      "2.01085823182421\n",
      "2.0109090034185324\n",
      "2.010958779491398\n",
      "2.011007436703865\n",
      "2.0110550191055543\n",
      "2.0111016412254843\n",
      "2.0111472942537305\n",
      "2.011191916520821\n",
      "2.011235631365699\n",
      "2.011278385928817\n",
      "2.011320303549117\n",
      "2.0113612520777324\n",
      "2.011401381283379\n",
      "2.0114406118767367\n",
      "2.0114790143372003\n",
      "2.011516571044922\n",
      "2.011553378909069\n",
      "2.011589367450247\n",
      "2.0116245630982292\n",
      "2.0116590363324103\n",
      "2.011692734293244\n",
      "2.01172573627005\n",
      "2.0117580686926018\n",
      "2.0117896610815036\n",
      "2.011820583916151\n",
      "2.011850837196544\n",
      "2.01188045616238\n",
      "2.0119094320037347\n",
      "2.0119377823404574\n",
      "2.0119655247923958\n",
      "2.0119926505496264\n",
      "2.0120192212816197\n",
      "2.0120452105586026\n",
      "2.0120706448103483\n",
      "2.0120955416567057\n",
      "2.012119909907599\n",
      "2.012143705513406\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "best_accuracy = 0\n",
    "for t in range(epochs):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_accuracy, train_loss = test(train_dataloader, model, loss_fn)\n",
    "    print(train_accuracy)\n",
    "    val_accuracy, val_loss =  test(test_dataloader, model, loss_fn)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if t % 50 == 0:\n",
    "        print(train_accuracy)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"At Epoch {t} model has Accuracy: {(100*val_accuracy):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\\\n",
    "        \n",
    "    if val_accuracy > best_accuracy:\n",
    "        print(f\"At Epoch {t} new best model found with Accuracy: {(100*val_accuracy):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "        best_accuracy = val_accuracy\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
